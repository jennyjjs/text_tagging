# -*- coding: utf-8 -*-
"""Text Tagging - Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/196hGncTLwIY2piP-ZIIptqCGR_Zfs88m
"""

from google.colab import drive
drive.mount('/content/drive')

## Import necessary packages.

import pandas as pd
import numpy as np
import string
import re
from collections import Counter
import nltk
# nltk.download('stopwords')
# nltk.download('punkt')
# import fastText
from nltk import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

"""# Import Data

Combine Sample_Data2.csv
"""

## Import data1

df = pd.read_excel("/content/drive/MyDrive/Text Analytics/Sample_Data.xlsx", sheet_name = "Sheet 1")
df.head()

df.shape

# Import data 2

df2 = pd.read_excel("/content/drive/MyDrive/Text Analytics/Sample_Data_2.xlsx", sheet_name = "Sheet 1")
df2.head()

# We only want the sections after Sub-classification. Remove the first 3 columns

df2_new = df2.iloc[:,3:]

print(df2_new.shape)
df2_new.head()

# Combine the 2 dfs 

df_final = df.append(df2_new).reset_index(drop = True)
df_final.shape

"""### Preprocess Text"""

## Duplicate keywords

for i in range(2, 41):
  if i == 2:
    df_final["Variable "+str(i)] = df_final.apply(
      lambda row: row["Variable " + str(i-1)] if pd.isnull(row["Variable " + str(i)]) else row["Variable "+ str(i)],
      axis=1)
  else:
    df_final["Variable "+str(i)] = df_final.apply(
      lambda row: row["Variable " + str(i-2)] if pd.isnull(row["Variable " + str(i)]) else row["Variable "+ str(i)],
      axis=1)

## Make everything lowercase and divide train test

df_lower_train = df_final.apply(lambda x: x.astype(str).str.lower()).iloc[:,0:35]
df_lower_test = df_final.apply(lambda x: x.astype(str).str.lower()).iloc[:,35:]
df_lower_test["Line item"] = df_final["Line item"].apply(lambda x: x.lower())

## Create a keywords column that captures all the keywords

df_lower_train["keywords"] = df_lower_train.astype(str).apply(" ".join, axis = 1)
df_lower_test["keywords"] = df_lower_test.astype(str).apply(" ".join, axis = 1)

## Get the columns needed

df_clean_train = df_lower_train[["Line item","keywords"]]
df_clean_test = df_lower_test[["Line item","keywords"]]

df_clean_train.head()

df_clean_test.head()

nltk.download('stopwords')

## Remove stopwords

stop = list(set(stopwords.words('english')))
stop.extend(["nan",",","","'",".","/","-"])

nltk.download('punkt')

keyword_token = []
for i in df_clean_train["keywords"]:
    keyword_token.append(word_tokenize(i))

df_clean_train["keyword_token"] = keyword_token

keyword_token = []
for i in df_clean_test["keywords"]:
    keyword_token.append(word_tokenize(i))

df_clean_test["keyword_token"] = keyword_token

stop_removed_keyword = []

for keyword in df_clean_train["keyword_token"]:
    words = []
    for word in keyword:
        if word not in stop:
            words.append(word)
    stop_removed_keyword.append(words)

desc = []
for i in stop_removed_keyword:
    temp = " ".join(i)
    desc.append(temp)
    
df_clean_train["stop_removed_keyword"] = desc

stop_removed_keyword = []

for keyword in df_clean_test["keyword_token"]:
    words = []
    for word in keyword:
        if word not in stop:
            words.append(word)
    stop_removed_keyword.append(words)

desc = []
for i in stop_removed_keyword:
    temp = " ".join(i)
    desc.append(temp)
    
df_clean_test["stop_removed_keyword"] = desc

df_clean_train.head()

df_clean_test.head()

"""## Logistic Model

### TF-IDF
"""

## We assign the labels and document. The document will be input data and the label is what we are trying to predict.

labels = df_clean_train["Line item"]
docs = df_clean_train["stop_removed_keyword"]

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics import accuracy_score, roc_curve
# from sklearn import metrics

tfidf_vectorizer=TfidfVectorizer()
X_train = tfidf_vectorizer.fit_transform(docs)

y_train = df_clean_train["Line item"].values
y_test = df_clean_test["Line item"].values

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

X_test = tfidf_vectorizer.transform(df_clean_test["stop_removed_keyword"])

y_pred = logreg.predict(X_test)

scores =[]
accs =[]
AUCs = []
tprs = []
mean_fpr = np.linspace(0, 1, 100)

results = logreg.fit(X_train, y_train)
scores.append(results.score(X_train, y_train))
accs.append(accuracy_score(y_test, results.predict(X_test)))
probas_ = results.predict_proba(X_test)
print("mean score: {}\nmean acc: {}".format(np.mean(scores), np.mean(accs)))

test_word = tfidf_vectorizer.transform(["hvac"])
logreg.predict(test_word)

"""- Incorporate Fuzzy matching
- spellcheck as part of pipeline
- Establish a "other" category
"""



"""### FastText"""

from gensim.models import FastText

train_emb = [word_tokenize(x) for x in df_clean_train["stop_removed_keyword"]]

model_emb = FastText(train_emb, size=500, window=5, min_count=2)

def document_vector(doc):
    """Create document vectors by averaging word vectors. Remove out-of-vocabulary words."""
    doc = [word for word in doc if word in model_emb.wv.vocab]
    return np.mean(model_emb[doc], axis=0)

df_clean_train["doc_vector"] = ""

for i in range(len(df_clean_train["stop_removed_keyword"])):
  doc = word_tokenize(df_clean_train["stop_removed_keyword"][i])
  df_clean_train["doc_vector"][i] = document_vector(doc)

df_clean_test["doc_vector"] = ""

for i in range(len(df_clean_test["stop_removed_keyword"])):
  doc = word_tokenize(df_clean_test["stop_removed_keyword"][i])
  df_clean_test["doc_vector"][i] = document_vector(doc)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_train = encoder.fit_transform(df_clean_train["Line item"].values)

X_train = list(df_clean_train['doc_vector'])
X_test = list(df_clean_test['doc_vector'])

logreg2 = LogisticRegression()
logreg2.fit(X_train, y_train)

y_pred_vec = logreg2.predict(X_test)
y_pred = encoder.inverse_transform(y_pred_vec)

scores =[]
accs =[]
AUCs = []
tprs = []
mean_fpr = np.linspace(0, 1, 100)

results = logreg2.fit(X_train, y_train)
scores.append(results.score(X_train, y_train))
accs.append(accuracy_score(y_test, results.predict(X_test)))
probas_ = results.predict_proba(X_test)
print("mean score: {}\nmean acc: {}".format(np.mean(scores), np.mean(accs)))

"""Another model:

TRIE classifier
- Tree with individual letters

Gaussian classifiers
- Naive Bayes
"""